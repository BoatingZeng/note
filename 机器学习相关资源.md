**ctrl + f is fun !**

# 比较好的blog或者系统的教程

* http://blog.csdn.net/zouxy09
* http://blog.pluskid.org
* http://blog.csdn.net/baimafujinji
* http://www.cnblogs.com/en-heng
* http://ruder.io/
* http://mccormickml.com/tutorials/
* https://r2rt.com/
* https://blog.acolyer.org/
* http://www.crownpku.com
* http://lamda.nju.edu.cn/weixs/
* https://codesachin.wordpress.com
* https://ckmarkoh.github.io/
* http://crescentmoon.info
* https://iwringer.wordpress.com/
* http://www.rossgirshick.info/
* https://oneraynyday.github.io/
* **NN**：
    * https://mlnotebook.github.io/
    * https://www.gitbook.com/book/tigerneil/neural-networks-and-deep-learning-zh/details
    * http://neuralnetworksanddeeplearning.com/about.html
* **深度学习论文集**：
    * https://github.com/scofield7419/Deep-Learning-Papers-for-Fish
    * https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap

* stanford的NLPwithDL：http://web.stanford.edu/class/cs224n/syllabus.html
* 各种优化器介绍：http://ruder.io/optimizing-gradient-descent/

# 一些代码例子

* ACL2017的一篇中文分词文章的代码：https://github.com/jcyk/greedyCWS

## Keras
* fine-tune image classification：https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
* fine tune in keras：https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html
* preprocess_input：https://deeplearningsandbox.com/how-to-build-an-image-recognition-system-using-keras-and-tensorflow-for-a-1000-everyday-object-559856e04699
* fine tune、preprocess_input：https://deeplearningsandbox.com/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2

**preprocess_input：在训练类似ImageNet的数据集时，应该使用preprocess_input，因为ImageNet的模型训练时，都利用了整个ImageNet均值或变准差这样的信息，另外，caffee使用的是BGR，resnet50和vgg16的weights都是从caffee转过来的，所以keras的resnet50和vgg16都用BGR。猜测在训练对颜色不敏感的数据集时，应该不用在意RGB和BGR。另外，如果fine tune的数据集和ImageNet的数据差异很大(比如说不是猫狗飞机车，而是医学影像)，用ImageNet的均值和标准差处理数据，应该是没有意义的。**

## BILSTM_CRF
* https://github.com/chilynn/sequence-labeling
* https://github.com/scofield7419/sequence-labeling-BiLSTM-CRF
* https://mp.weixin.qq.com/s?__biz=MjM5ODIzNDQ3Mw==&mid=2649966433&idx=1&sn=be6c0e5485003d6f33804261df7c3ecf&chksm=beca376789bdbe71ef28c509776132d96e7e662be0adf0460cfd9963ad782b32d2d5787ff499&mpshare=1&scene=1&srcid=1122cZnCbEKZCCzf9LOSAyZ6&pass_ticket=lT4VaDjNiXiIPNmtxEJuioi434%2Bhm9W7at4S93hYP0U%3D#rd
* http://blog.csdn.net/appleml/article/details/70945298
* https://github.com/yanshao9798/tagger

# 按主题分类

## CNN和RNN的结合：
* 音频处理：https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/#combinations-of-cnn-and-rnn
* 视频处理：http://qr.ae/TUpnIY

## 推荐系统、广告、点击率

### FFM(Field-aware Factorization Machine)
* https://blog.csdn.net/happytofly/article/details/80122160
* https://github.com/daxiongshu/light-ffm
* https://github.com/guestwalk/libffm

## 计算机视觉(computer vision、CV)、图像处理

### 傅立叶变换
* 傅立叶级数：https://www.zhihu.com/question/19991026/answer/252715189
* 二维离散傅立叶变换：
    * https://www.jianshu.com/p/7dfe02fa34a9
    * https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.html
    * https://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm

## 强化学习(Reinforcement Learning)
* https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/
* Q-Learning：https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-2-A-q-learning/
* Policy Gradient：https://blog.csdn.net/amds123/article/details/70242042
* Actor-Critic：
    * https://www.cnblogs.com/wangxiaocvpr/p/8110120.html
    * https://blog.csdn.net/jinzhuojun/article/details/72851548

## Global Average Pooling (GAP)
* https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/

## 异常检测
* https://www.kaggle.com/victorambonati/unsupervised-anomaly-detection/notebook

## 目标检测(Object Detection)
* Mask R-CNN：https://github.com/facebookresearch/Detectron
* Mask R-CNN：https://zhuanlan.zhihu.com/p/34816076
* R-CNN、Fast R-CNN、Faster R-CNN：https://www.cnblogs.com/skyfsm/p/6806246.html

## 异常检测
* https://www.zhihu.com/question/57072166/answer/280824223
* https://iwringer.wordpress.com/2015/11/17/anomaly-detection-concepts-and-techniques/

## 迁移学习
* https://zhuanlan.zhihu.com/p/27657264

## NLP
* http://blog.csdn.net/sinat_26917383/article/details/55683599
* https://zhuanlan.zhihu.com/p/28126584
* https://zhuanlan.zhihu.com/p/24833012

### 分词
* http://blog.csdn.net/aiwuzhi12/article/details/54707695

# 一些概念或算法或模型的解释

## ROC曲线
* https://www.deeplearn.me/1522.html

## 信息熵
* https://zhuanlan.zhihu.com/p/26486223

## 条件熵
* https://zhuanlan.zhihu.com/p/26551798

## 梯度下降法
* http://blog.csdn.net/zouxy09/article/details/20319673

## 最大似然和EM
* http://blog.csdn.net/zouxy09/article/details/8537620

## GBM|Boosted Trees|XGBoost|GBDT|GBT|GTB(《统计学习方法》第8章)
* https://xgboost.readthedocs.io/en/latest/model.html
* https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf
* https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/
* 梯度提升树中为什么说目标函数关于当前模型的负梯度是残差的近似值？ - 李云聪的回答 - 知乎：https://www.zhihu.com/question/60625492/answer/345471640
* Feature Importance：https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/

## SVM
* http://blog.csdn.net/zouxy09/article/details/17291543
* http://blog.pluskid.org/?page_id=683

### SMO
* https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html
* https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf
* http://blog.csdn.net/on2way/article/details/47730367
* http://blog.csdn.net/v_july_v/article/details/7624837

### KKT
在SVM中会提及，但不是SVM的专有概念
* http://www.onmyphd.com/?p=kkt.karush.kuhn.tucker

## HMM
* https://www.cnblogs.com/skyme/p/4651331.html

## CRF
* http://www.jianshu.com/p/55755fc649b1

* CRF++的解析，里面有对特征模版的解析：http://www.52nlp.cn/中文分词入门之字标注法4

* 分词实例：
    * http://www.52nlp.cn/初学者报道3-crf-中文分词解码过程理解
    * http://www.cnblogs.com/liufanping/p/4899842.html

* 这个比较难懂，下面是相关资料集合
    * http://www.52nlp.cn/条件随机场文献阅读指南
    * http://www.inference.org.uk/hmw26/crf/

## word2vec
* http://blog.csdn.net/mytestmy/article/details/26961315
* https://www.cnblogs.com/iloveai/p/word2vec.html
* https://www.tensorflow.org/tutorials/word2vec
* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

## RNN
* http://karpathy.github.io/2015/05/21/rnn-effectiveness/
* 在tensorflow中：https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767
* 上文第一篇的翻译：https://zhuanlan.zhihu.com/p/26646665
* https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html

### LSTM
* http://www.jianshu.com/p/9dc9f41f0b29
* http://colah.github.io/posts/2015-08-Understanding-LSTMs/

### seq2seq
* 基础seq2seq模型：https://zhuanlan.zhihu.com/p/27608348

## N-Gram
* http://blog.csdn.net/baimafujinji/article/details/51281816

## CNN
* http://blog.csdn.net/yunpiao123456/article/details/52437794
* http://www.cnblogs.com/fengfenggirl/p/cnn_implement.html

## L1、L2正则化
L1、L2以约束条件的形式给出比较好理解。SVM里面的那个松弛项，最后也是转化为约束条件，对SVM而言，超参数C越大，对离群点的容许越小，可以这么考虑，极端的，如果C为0，则无论离群多大，后面的惩罚项都是0，也就是对离群无限容许。数学上，感觉“惩罚项”和“松弛项”是差不多的。
* http://blog.csdn.net/jinping_shi/article/details/52433975
* http://blog.csdn.net/vividonly/article/details/50723852
* bias-variance-tradeoff：https://liam0205.me/2017/03/25/bias-variance-tradeoff/

## 降维相关

### SNE和t-SNE
* http://www.datakit.cn/blog/2017/02/05/t_sne_full.html
* https://distill.pub/2016/misread-tsne/

### PCA
* http://blog.jobbole.com/109015/

### SVD
* https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html

### 线性判别分析LDA(Linear Discriminant Analysis)
* https://www.cnblogs.com/pinard/p/6244265.html

**↑↓这两个LDA是不同概念的缩写**

### LDA(Latent Dirichlet Allocation)主题模型
* http://blog.csdn.net/aws3217150/article/details/53840029
* http://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6

## 变分推理(Variational Inference)(详细看PRML第十章)
* https://ckmarkoh.github.io/blog/2016/06/20/pgm-variational-inference/
* http://crescentmoon.info/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/

## 吉布斯采样(Gibbs Sampling)
* https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling

# 一些工具和库

## OpenCV
* OpenCV里的DNN：https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/

## fasttext
* https://zhuanlan.zhihu.com/p/27699559

## Familia
* https://github.com/baidu/Familia/wiki

## TensorFlow

* 相关资源：
    * https://danijar.com/

## pytorch

* CrossEntropyLoss和NLLLoss：http://sshuair.com/2017/10/21/pytorch-loss/

## spotlight(推荐系统)
https://github.com/maciejkula/spotlight

* 包含传统的协同过滤方法
* 包含了DNN方法，这些方法都是把推荐问题当作序列预测处理，使用用户的历史信息作为一个序列，方法论文如下：
    * YouTube的pooling方法，这里实现的是一个简化版：https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf
    * LSTM：https://arxiv.org/pdf/1511.06939
    * causal convolution(空洞卷积)：https://arxiv.org/pdf/1609.03499

# 论文集（懒得下载）

## 机器学习在物理学方面
* 流体力学
    * Deep learning in fluid dynamics：https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/deep-learning-in-fluid-dynamics/F2EDDAB89563DE5157FC4B8342AD9C70/core-reader
    * Machine Learning Methods for Data-Driven Turbulence Modeling：http://sci-hub.tw/10.2514/6.2015-2460

* Submanifold Sparse Convolutional Networks：https://arxiv.org/pdf/1706.01307.pdf

## DCNN(IDCNN)
* MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS：https://arxiv.org/pdf/1511.07122.pdf
* Fast and Accurate Entity Recognition with Iterated Dilated Convolutions：https://arxiv.org/pdf/1702.02098.pdf

## Batch Normalization
* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift：https://arxiv.org/pdf/1502.03167.pdf
* 关于train和inference时的BN的具体操作：https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b
* 关于BN和dropout混合使用的不协调：https://arxiv.org/pdf/1801.05134.pdf

## NER
* Character-Based LSTM-CRF with Radical-Level
Features for Chinese Named Entity Recognition：https://pdfs.semanticscholar.org/b944/5206f592423f0b2faf05f99de124ccc6aaa8.pdf

### 嵌套NER(nested entity 或 composite entity)

微软的luis、facebook的wit、谷歌的dialogflow都有composite entity。

* Nested Named Entity Recognition：https://nlp.stanford.edu/pubs/nested-ner.pdf
* 训练两个模型，一个是普通无nested，第二个是nested，第二个模型把无nested的NER信息作为特征：https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2014/2014_GermEval_Nested_Named_Entity_Recognition_with_Neural_Networks.pdf

## CWS
* Fast and Accurate Neural Word Segmentation for Chinese：https://arxiv.org/pdf/1704.07047.pdf
* Neural Word Segmentation Learning for Chinese：https://arxiv.org/pdf/1606.04300.pdf

## 其他
* seq2seq模型是否学习到了语义特征(Does String-Based Neural MT Learn Source Syntax?)：https://www.isi.edu/natural-language/mt/emnlp16-nmt-grammar.pdf
* 序列异常检测(Long Short Term Memory Networks for
Anomaly Detection in Time Series)：https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf

# 语料资源

## 别人的索引
* https://github.com/crownpku/awesome-chinese-nlp#corpus-%E4%B8%AD%E6%96%87%E8%AF%AD%E6%96%99
* https://github.com/candlewill/Dialog_Corpus

## 某些下载页面
* ubuntu-corpus-1.0：http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/
* 病历NER：https://github.com/tangxiangru/CCKS2017
* http://111.200.194.212/cqp/
* http://www.lancaster.ac.uk/fass/projects/corpus/
* https://nlp.stanford.edu/links/statnlp.html

# 各种数据集

## 手
* https://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm#download
* http://cvrr.ucsd.edu/vivachallenge/index.php/hands/hand-detection/
* http://lttm.dei.unipd.it/downloads/gesture/#kinect_leap

## 微软Kinect
* https://github.com/shahroudy/NTURGB-D
* http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp

# 其他

## HTM-CLA(Hierarchical Temporal Memmory Cortical Learning Algorithm)
HTM-CLA是另一个智能研究的方向。

* https://numenta.com
* https://numenta.org
* https://github.com/numenta/nupic
* 关注的人不多，贴一个中文blog：https://blog.csdn.net/jiede1/article/details/79240966
* HTM-CLA白皮书中文版：https://numenta.com/assets/pdf/whitepapers/hierarchical-temporal-memory-cortical-learning-algorithm-0.2.1-cn.pdf
* numenta对各种AI的解释：https://numenta.com/blog/2016/01/11/machine-intelligence-machine-learning-deep-learning-artificial-intelligence/