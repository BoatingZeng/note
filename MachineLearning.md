# 机器学习

## 准确率、召回率、F1-score
对于数据测试结果有下面4种情况：

table   | 预测为正 | 预测为负
--------|----------|--------
实则为正 |   TP     |   FN
实则为负 |   FP     |   TN

* 准确率：P = TP/(TP+FP)，就是预测为正的有多少(百分比)是正，分母是上表第一列求和
* 召回率：R = TP/(TP+FN)，就是真实是正的预测出多少(百分比)，分母是上表第一行求和
* F1-score：F = 2PR/(P+R)

### 对于分词

分词问题转化为位置标注问题(通常是B、M、E、S标注)，所以评价就是通过标注的准确性来评价，每种标注都可以单独算出准确率和召回率，单独考虑每种标注，就可以套上面的公式了。下面是一个简化的测试文本(没有M标注)。第二列是正确答案，第三列是预测答案。看下面计算过程的注释部分。

```txt
这  S  S
是  S  S
一  B  B
个  E  E
测  B  S
试  E  S
文  B  B
本  E  E
```

```python
import pandas
line=[]
file=open('./result.txt','r',encoding='utf-8')
for i in file.readlines():
    i=i[0:-1]
    if len(i)!=0 and len(i)!=1:
        line.append(i.split())
df=pandas.DataFrame(line,columns=['character','train','test'])

print(df)

correct=df[df.train==df.test]

print(correct)

for i in ('B','E','S'):
    # 分别对B、E、S计算，下面以B为例注释
    # 分子：你预测到的而且是正确的B。分母：你预测到的全部B，就是上表的第一列求和。
    P=sum(correct.test==i)/sum(df.test==i)
    # 分子：一样。分母：正确答案里所有的B，就是上表的第一行求和。
    R=sum(correct.test==i)/sum(df.train==i)
    F=R*P*2/(R+P)
    print(i,':\n','P=',P,' R=',R,' F=',F)
```

计算结果：

table |  P  |  R  |  F
------|-----|-----|-----
B     | 1.0 |0.667| 0.8
E     | 1.0 |0.667| 0.8
S     | 0.5 | 1.0 | 0.667

## 维特比算法

参考吴军的《数学之美》

S -> x1 -> x2 -> xn

x1、x2、xn各有k1、k2、kn个节点。节点命名为xij，i=1~n，j=1~ki。

1. 计算S到x1的k1个节点**各自**的最短距离，就是要得出k1个距离(对于x1，直接就是最短距离，没有选择)
2. 计算S到x2的k2个节点**各自**的最短距离，不是比较x2的k2个节点之间的最短，而是以x2的k2个节点为终点，计算到它们的最短距离，也就是会计算出k2个距离。计算方法参考书上P231。
3. 递推，就得到从S到每一步xi的每一个节点xij的最短距离。这个已经是我们想要的结果。最短距离就是最后一步xn里面，距离最短的那个节点所表示的距离，路径只要每一步都记下来就行。

找到S到xi的各个节点的最短距离的意义在于，当我们从xi走到xi+1时，如果最短距离经过xi的某个节点xij，那么S到xij的距离肯定是S到xi的最短距离。这里并不是说一定要经过xij，而是如果要经过，那么它必须在所有S到xij的可能路径中最短。

把过程倒过来说，如果已经要从n-1步到n步(最后一步)，要求出最短距离，因为S到n-1步每个节点的最短距离已经得出，所以只要用这些节点往第n步的所有节点都走一次，做一个加法，再比较，就可以找出到第n步的最短距离。

## CRF

这篇paper对理解CRF很有帮助：Classical Probabilistic Models and Conditional Random Fields。http://www.eng.utah.edu/~cs6961/papers/klinger-crf-intro.pdf

**CRF就是最大熵模型的序列化扩展，是HMM的条件求解**，基本搜到的资料都会这么说，但是看完之后才会明白为什么这样说，还有**生成模型**和**判别模型**的区别。

看完这个，细节上就差特征函数具体怎么操作(不讨论训练的方式或细节)。下面参考CRF++的做法，估计多数实现都是类似的。

### CRF++的特征模版解析
下面是直接拿CRF++的example来说

**template文件**

```t
# Unigram
U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-2,0]/%x[-1,0]/%x[0,0]
U06:%x[-1,0]/%x[0,0]/%x[1,0]
U07:%x[0,0]/%x[1,0]/%x[2,0]
U08:%x[-1,0]/%x[0,0]
U09:%x[0,0]/%x[1,0]

# Bigram
B
```

这个模版，是用来扫描句子的，扫描训练数据的句子，就是提取特征(函数)，扫描测试数据的句子，就是计算这些特征(函数)的值。

这里的Unigram和Bigram是**相对标注而言**的，不是相对字(特征)本身而言的，这点很重要。可以这么理解，Unigram就是提取某个位置的标注的特征，Bigram就是提取相邻两个标注的特征，可以扩展到更多更复杂。上面的Bigram只有一个B模版，就是说只考虑相邻两个标注的转移概率，这是很简单的情形。下面重点说Unigram。Bigram理解为HMM模型中的转移概率就行。

[-2,0]这种表示，-2表示训练数据中的行(相对当前位置，因为模版是在句子上移动的)，0表示列(特征)，每个特征模版都会扫描整个句子，被扫到的字本身就是[0,0]。比如模版U02:%x[0,0]扫描到“每”字时，U02:%x[0,0]=U02:每。下面的解析会以U02:%x[0,0]这个模版为例。

U00、U01这些都是用来给模版命名，或者说编号的。

**训练数据样板train**

```t
毎	k	B
日	k	I
新	k	I
聞	k	I
社	k	I
特	k	B
別	k	I
顧	k	B
問	k	I
４	n	B
氏	k	B
の	h	B
略	k	B
歴	k	I
```

除了最后一列，其余列都可以认为是特征，最后一列是标注，因为上面的特征模版没有用到第二列，所以下面的讨论也不考虑第二列，而且一般来说，分词也只有字本身作为特征。这个例子里只有两种标注(B、I)。

当U02:%x[0,0]这个模版扫描上面这些数据时，在模型里(需要导出文本型模型)，会得到：

```t
4 U02:毎
6 U02:日
8 U02:新
10 U02:聞
...
...后面省略
```

第一列数字是编号，注意到是隔了2，因为有两种标注。模型文件里的上面4行，会生成下面这样的特征函数：

```
func1 = if (output = B and feature="U02:毎") return 1 else return 0
func2 = if (output = I and feature="U02:毎") return 1 else return 0

func3 = if (output = B and feature="U02:日") return 1 else return 0
func4 = if (output = I and feature="U02:日") return 1 else return 0

func5 = if (output = B and feature="U02:新") return 1 else return 0
func6 = if (output = I and feature="U02:新") return 1 else return 0

func7 = if (output = B and feature="U02:聞") return 1 else return 0
func8 = if (output = I and feature="U02:聞") return 1 else return 0
```

上面这些特征函数，接受两个输入(output和feature)，output就是标注，feature就是抽取的特征。在预测时，用同一个特征模版，在被预测的数据上提取特征，然后再代入特征函数，当然，猜测的标注也要代入，就可以计算出结果了。

**下面说的特征函数和权值，其实就是拉格朗日成子法里的约束条件和拉格朗日乘子**

这样思考一下，训练数据是用来干什么的？某种程度上，就是给我们提供约束的，不然我们漫无目的地在模型空间里寻找，天黑也找不到那个理想模型。

特征函数已经生成完毕，生成特征函数的过程只是个简单的扫描，其实是提取特征的过程。训练时主要计算工作是，计算每个特征函数的权值。权值是用来和特征函数相乘的，特征函数一般只输出1或0(因为简单)，但并不限制特征函数的形式(可以是连续函数，输出可以是任意数值)。

假设已经训练出每个特征函数的权值。下面给一个句子，进行预测(分词就是一个预测过程，预测这些字序列的标注序列)。

假设现在要预测上面用来训练的句子(这里只是举例)。

假设现在有两种标注方式，用U02:%x[0,0]特征模版在这个句子上扫描一次。两种标注方式计算出的8个特征函数的结果，是不同的，正确的标注计算出来的是10010101，错误的计算出来的是01011001。这里为了方便，每一位写一个函数的计算结果，共8位。然后每一位的值和各自对应的权相乘，再求和，如果模型是正确的话，那么正确的标注计算出的这个和，应该是比较大，所以我们才选择了正确的标注。实际计算中，并不是计算每种可能标注的和然后比较，而是用维特比算法来求，但是理解成暴力比较每种可能标注的结果，也是可以的，结果是一样，只是方法优劣问题。

正确的：

```t
毎	B
日	I
新	I
聞	I
```

错误的：

```t
毎	I
日	I
新	B
聞	I
```

上面的这些讨论，可以用来理解ltp的特征和模型文件的格式。当然，ltp的特征函数的形式，可能是更加复杂的。CRF++的模型里，是先把特征都列出来，再列出每个特征的权值，而ltp的模型里，权值就写在特征后面。

**ltp里特征裁剪参数rare-feature-threshold的说明**

原文：rare-feature-threshold：模型裁剪力度，如果rare-feature-threshold为0，则只去掉为0的特征；rare-feature-threshold；如果大于0时将进一步去掉更新次数低于阈值的特征。

特别说明当rare-feature-threshold为0的时候，是去掉**特征权值全部为0**的特征(这里说的全部，对于分词来说，一般是4，因为有4个标注)。ltp所提及的论文里说这个特征剪裁方法，跟单纯的根据特征出现次数来剪裁特征，是不同的。也就是说，特征的出现次数不等价于更新次数，细节要研究特征是怎样更新的。

另外，设置rare-feature-threshold大于0不会让训练更快，理解剪裁的工作机制就知道原因了。(相当于最后出模型的时候，根据rare-feature-threshold来选择性导出模型的哪些部分)